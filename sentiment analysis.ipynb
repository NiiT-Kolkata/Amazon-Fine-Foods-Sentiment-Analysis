{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/max/525/1*96kuTKGS6y9_gf7MRS2gHw.png\" width=\"900\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://miro.medium.com/max/525/1*96kuTKGS6y9_gf7MRS2gHw.png\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(url= \"https://miro.medium.com/max/525/1*96kuTKGS6y9_gf7MRS2gHw.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USEFUL LINKS/CITATION - \n",
    "\n",
    "https://nycdatascience.com/blog/student-works/amazon-fine-foods-visualization/\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective:\n",
    "\n",
    "To do Sentiment Analysis and determine whether the Reviews from Amazon are Negative (rating of 1 or 2), Positive (rating of 4 or 5) or Neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVERVIEW\n",
    "Sentiment analysis is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.\n",
    "\n",
    "Since customers express their thoughts and feelings more openly than ever before, sentiment analysis is becoming an essential tool to monitor and understand that sentiment. Automatically analyzing customer feedback, such as opinions in survey responses and social media conversations, allows brands to learn what makes customers happy or frustrated, so that they can tailor products and services to meet their customers’ needs. For example, using sentiment analysis to automatically analyze 4,000+ reviews about your product could help you discover if customers are happy about your pricing plans and customer service.\n",
    "\n",
    "Ques- How to determine if a review is positive or negative?\n",
    "\n",
    "[Ans] We could use the Score/Rating. A rating of 4 or 5 could be cosnidered a positive review. A review of 1 or 2 could be considered negative. A review of 3 is nuetral and ignored. This is an approximate and proxy way of determining the polarity (positivity/negativity) of a review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Natural Language?\n",
    "\n",
    "Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n",
    "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n",
    "\n",
    "Natural language processing includes many different techniques for interpreting human language, ranging from statistical and machine learning methods to rules-based and algorithmic approaches. We need a broad array of approaches because the text- and voice-based data varies widely, as do the practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARTIFICIAL NEURAL NETWORK(ANN)\n",
    "An artificial neural network (ANN) is the component of artificial intelligence that is meant to simulate the functioning of a human brain.\n",
    "Processing units make up ANNs, which in turn consist of inputs and outputs. The inputs are what the ANN learns from to produce the desired output.\n",
    "\n",
    "EXAMPLES - Commercial applications of these technologies generally focus on solving complex signal processing or pattern recognition problems. Examples of significant commercial applications since 2000 include handwriting recognition for check processing, speech-to-text transcription, oil-exploration data analysis, weather prediction and facial recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TYPES\n",
    "Fine-grained Sentiment Analysis\n",
    "If polarity precision is important to your business, you might consider expanding your polarity categories to include:\n",
    "\n",
    "1.Very positive\n",
    "\n",
    "2.Positive\n",
    "\n",
    "3.Neutral\n",
    "\n",
    "4.Negative\n",
    "\n",
    "5.Very negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is spaCy or NLTK better?\n",
    "\n",
    "While NLTK provides access to many algorithms to get something done, spaCy provides the best way to do it. It provides the fastest and most accurate syntactic analysis of any NLP library released to date. It also offers access to larger word vectors that are easier to customize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHY IS IT IMPORTANT?\n",
    "Sentiment analysis is extremely useful in social media monitoring as it allows us to gain an overview of the wider public opinion behind certain topics. Being able to quickly see the sentiment behind everything from forum posts to news articles means being better able to strategise and plan for the future. companies want their brand being perceived positively, or at least more positively than the brands of competitors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC STEPS\n",
    "The common and most basic steps are:\n",
    "\n",
    "1.Remove URLs and email addresses from every single sample — because they won’t add meaningful value.\n",
    "\n",
    "2.Remove punctuation signs — otherwise your model won’t understand that “good!” and “good” are actually meaning the same thing.\n",
    "\n",
    "3.Lowercase all text — because you want to make the input text as generic as possible and avoid that, for example, a “Good” which is at the beginning of a phrase to be understood differently than the “good” in another sample.\n",
    "\n",
    "4.Remove stop-words — stop-words refer to the most common words in a language, such as “I”, “have”, “are” and so on. I hope you get the point because there’s not an official stop-words list out there.\n",
    "\n",
    "5.Stemming/Lemmatizing:  Lemmatizing generally returns valid words (that exist) while stemming techniques return (most of the times) shorten words, that’s why lemmatizing is used more in real world implementations. This is how lemmatizers vs. stemmers work: suppose you want to find the root word of ‘caring’: ‘Caring’ -> Lemmatization -> ‘Care’. In the other hand: ‘Caring’ -> Stemming -> ‘Car’; did you get the point? You can research about both and obviously implement any if the business requires it.\n",
    "\n",
    "6.Transform dataset (text) into numeric tensors — Usually referred as vectorization.like all other neural networks, deep-learning models don’t take as input raw text: they only work with numeric tensors, that’s why this step is not negotiable. There are multiple ways to do so; for example, if you’re going to use a classic ML model (not DL) then you definitely should go with CountVectorizer, TFIDF Vectorizer or just the basic but not so good approach: Bag-Of-Words. It’s up to you. However, if you’re going to implement Deep Learning you might know that the best way is to turn your text data (that can be understood as sequences of word or sequences of characters) into low-dimensional floating-point vectors \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/max/650/1*P9agr-brgGGbTCjA7PEBvQ.png\" width=\"600\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://miro.medium.com/max/650/1*P9agr-brgGGbTCjA7PEBvQ.png\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOUT DATASET\n",
    "Context\n",
    "This dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories.\n",
    "\n",
    "Contents\n",
    "Reviews.csv: Pulled from the corresponding SQLite table named Reviews in database.sqlite\n",
    "\n",
    "database.sqlite: Contains the table 'Reviews'\n",
    "\n",
    "Data includes:\n",
    "\n",
    "Reviews from Oct 1999 - Oct 2012\n",
    "\n",
    "568,454 reviews\n",
    "\n",
    "256,059 users\n",
    "\n",
    "74,258 products\n",
    "\n",
    "260 users with > 50 reviews\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "Id\n",
    "\n",
    "ProductId - unique identifier for the product\n",
    "\n",
    "UserId - unqiue identifier for the user\n",
    "\n",
    "ProfileName\n",
    "HelpfulnessNumerator - number of users who found the review helpful\n",
    "\n",
    "HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n",
    "\n",
    "Score - rating between 1 and 5\n",
    "\n",
    "Time - timestamp for the review\n",
    "\n",
    "Summary - brief summary of the review\n",
    "\n",
    "Text - text of the review\n",
    "\n",
    "Dataset source : https://www.kaggle.com/snap/amazon-fine-food-reviews?select=Reviews.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPACY\n",
    "\n",
    "Spacy is written in cython language, (C extension of Python designed to give C like performance to the python program). Hence is a quite fast library. spaCy provides a concise API to access its methods and properties governed by trained machine (and deep) learning models.\n",
    "\n",
    "Implementation of spacy and access to different properties is initiated by creating pipelines. A pipeline is created by loading the models. There are different type of models provided in the package which contains the information about language – vocabularies, trained vectors, syntaxes and entities.\n",
    "\n",
    "These pipelines outputs a wide range of document properties such as – tokens, token’s reference index, part of speech tags, entities, vectors, sentiment, vocabulary etc. \n",
    "\n",
    "a)Tokenization: Every spaCy document is tokenized into sentences and further into tokens which can be accessed by iterating the document.\n",
    "\n",
    "b)Part of Speech Tagging: Part-of-speech tags are the properties of the word that are defined by the usage of the word in the grammatically correct sentence. These tags can be used as the text features in information filtering, statistical models, and rule based parsing.\n",
    "\n",
    "c)Entity Detection Spacy consists of a fast entity recognition model which is capable of identifying entitiy phrases from the document. Entities can be of different types, such as – person, location, organization, dates, numerals, etc. These entities can be accessed through “.ents” property.\n",
    "\n",
    "d)Dependency Parsing One of the most powerful feature of spacy is the extremely fast and accurate syntactic dependency parser which can be accessed via lightweight API. The parser can also be used for sentence boundary detection and phrase chunking. The relations can be accessed by the properties “.children” , “.root”, “.ancestor” etc.\n",
    "\n",
    "e)Noun Phrases Dependency trees can also be used to generate noun phrases\n",
    "\n",
    "f)Word to Vectors Integration Spacy also provides inbuilt integration of dense, real valued vectors representing distributional similarity information. It uses GloVe vectors to generate vectors. GloVe is an unsupervised learning algorithm for obtaining vector representations for words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\work\\anaconda3\\lib\\site-packages (3.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (4.62.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (8.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (2.0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (3.10.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (1.7.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\work\\anaconda3\\lib\\site-packages (from catalogue<2.1.0,>=2.0.3->spacy) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\work\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\work\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.1.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\work\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\work\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\work\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\work\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\work\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Collecting click<7.2.0,>=7.1.1\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\work\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: click\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.0.1\n",
      "    Uninstalling click-8.0.1:\n",
      "      Successfully uninstalled click-8.0.1\n",
      "Successfully installed click-7.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of C:\\Users\\work\\anaconda3\\lib\\site-packages\\matplotlib-3.4.2-py3.7-nspkg.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\work\\anaconda3\\lib\\site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"<frozen importlib._bootstrap>\", line 580, in module_from_spec\n",
      "  AttributeError: 'NoneType' object has no attribute 'loader'\n",
      "\n",
      "Remainder of file ignored\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\work\\anaconda3\\lib\\site-packages (1.3.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of C:\\Users\\work\\anaconda3\\lib\\site-packages\\matplotlib-3.4.2-py3.7-nspkg.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\work\\anaconda3\\lib\\site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"<frozen importlib._bootstrap>\", line 580, in module_from_spec\n",
      "  AttributeError: 'NoneType' object has no attribute 'loader'\n",
      "\n",
      "Remainder of file ignored\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\work\\anaconda3\\lib\\site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\work\\anaconda3\\lib\\site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\work\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\work\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The plotly Python library is an interactive, open-source plotting library that supports over 40 unique chart types covering a wide range of statistical, financial, geographic, scientific, and 3-dimensional use-cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\users\\work\\anaconda3\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\work\\anaconda3\\lib\\site-packages (from plotly) (7.0.0)\n",
      "Requirement already satisfied: six in c:\\users\\work\\anaconda3\\lib\\site-packages (from plotly) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of C:\\Users\\work\\anaconda3\\lib\\site-packages\\matplotlib-3.4.2-py3.7-nspkg.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\work\\anaconda3\\lib\\site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"<frozen importlib._bootstrap>\", line 580, in module_from_spec\n",
      "  AttributeError: 'NoneType' object has no attribute 'loader'\n",
      "\n",
      "Remainder of file ignored\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\work\\anaconda3\\lib\\site-packages (3.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (1.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (8.0.7)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (2.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (3.10.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (4.62.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\work\\anaconda3\\lib\\site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\work\\anaconda3\\lib\\site-packages (from catalogue<2.1.0,>=2.0.3->spacy) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\work\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\work\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.1.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\work\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\work\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\work\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\work\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\work\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\work\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\work\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of C:\\Users\\work\\anaconda3\\lib\\site-packages\\matplotlib-3.4.2-py3.7-nspkg.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\work\\anaconda3\\lib\\site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"<frozen importlib._bootstrap>\", line 580, in module_from_spec\n",
      "  AttributeError: 'NoneType' object has no attribute 'loader'\n",
      "\n",
      "Remainder of file ignored\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\work\\anaconda3\\lib\\site-packages (1.19.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of C:\\Users\\work\\anaconda3\\lib\\site-packages\\matplotlib-3.4.2-py3.7-nspkg.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\work\\anaconda3\\lib\\site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"<frozen importlib._bootstrap>\", line 580, in module_from_spec\n",
      "  AttributeError: 'NoneType' object has no attribute 'loader'\n",
      "\n",
      "Remainder of file ignored\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\work\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1d9781c2ce58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHTML\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"<style>.container { width:100% !important; }</style>\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[1;31m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m \u001b[1;31m# linear algebra\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# numpy compat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m from pandas.compat import (\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mnp_version_under1p18\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_np_version_under1p18\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mis_numpy_dev\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_is_numpy_dev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\compat\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m from pandas.compat.numpy import (\n\u001b[0;32m     16\u001b[0m     \u001b[0mis_numpy_dev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;31m# array-like\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m \u001b[0mArrayLike\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ExtensionArray\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[0mAnyArrayLike\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mArrayLike\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Index\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Series\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'ndarray'"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np # linear algebra\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "import spacy\n",
    "from spacy import displacy #spaCy also comes with a built-in dependency visualizer that lets you check your model's predictions in your browse\n",
    "from spacy.util import minibatch, compounding\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go #Plotly's Python graphing library makes interactive, publication-quality graphs\n",
    "import plotly.tools as tls\n",
    "import plotly.express as px\n",
    "from nltk.stem import PorterStemmer #The Porter stemming algorithm (or 'Porter stemmer') is a process for removing the commoner morphological and inflexional endings from words in English\n",
    "!pip install textblob #TextBlob is a simple library which supports complex analysis and operations on textual data.\n",
    "from textblob import TextBlob\n",
    "import sqlite3\n",
    "import string\n",
    "# This is used for fast string concatination\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Gensim is a free open-source Python library for representing documents as semantic vectors, as efficiently (computer-wise) and painlessly (human-wise) as possible. Gensim is designed to process raw, unstructured digital texts (“plain text”) using unsupervised machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The sklearn. metrics module implements several loss, score, and utility functions to measure classification performance. Some metrics might require probability estimates\n",
    "#of the positive class, confidence values, or binary decisions values.\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc #The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance \n",
    "#of the model at distinguishing between the positive and negative classes.\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer #TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  #Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "from sklearn.feature_extraction.text import CountVectorizer  #Scikit-learn's CountVectorizer is used to convert a collection of text documents to a vector of term/token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types and counts for each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check size of dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain list of unique words in Summary\n",
    "total_options = df[\"Summary\"]\n",
    "total_options.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain list of unique words in Text\n",
    "total_options = df[\"Text\"]\n",
    "total_options.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(dataframe['Score'], palette='gist_rainbow')\n",
    "plt.title(\"Distribution of Ratings across the whole dataset\")\n",
    "plt.xlabel(\"Reviews Ratings \")\n",
    "plt.ylabel(\"Number of reviews corresponding to the 5 ratings\")\n",
    "plt.show()\n",
    "\n",
    "print(dataframe['Score'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Natural Language Toolkit (NLTK) is a platform used for building Python programs that work \n",
    "#with human language data for applying in statistical natural language processing (NLP). It contains text processing libraries \n",
    "#for tokenization, parsing, classification, stemming, tagging and semantic reasoning.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign reviews with score > 5 as positive sentiment\n",
    "# score < 5 negative sentiment\n",
    "# remove score = 5\n",
    "df = df[df['Score'] != 5]\n",
    "df['sentiment'] = df['Score'].apply(lambda rating : +1 if rating > 5 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is available in two forms\n",
    "\n",
    "1.csv file\n",
    "2.SQLite Database\n",
    "\n",
    "In order to load the data, I have used the SQLITE dataset as it easier to query the data and visualise the data and results efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcloud\n",
    "\n",
    "Word Cloud is a data visualization technique used for representing text data in which the size of each word indicates its frequency or importance. Significant textual data points can be highlighted using a word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"stream_name\":\"stdout\",\"time\":\"0:00:00.233244\",\"data\":\"Loading required package: DBI\\n\"},{\"stream_name\":\"stdout\",\"time\":\"0:00:00.237744\",\"data\":\"Loading required package: methods\\n\"},{\"stream_name\":\"stdout\",\"time\":\"0:00:00.641376\",\"data\":\"Loading required package: NLP\\n\"},{\"stream_name\":\"stdout\",\"time\":\"0:00:00.783783\",\"data\":\"Loading required package: RColorBrewer\\n\"},{\"stream_name\":\"stdout\",\"time\":\"0:00:34.382261\",\"data\":\"null device \\n\"},{\"stream_name\":\"stdout\",\"time\":\"0:00:34.382390\",\"data\":\"          1 \\n\"},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "df2=final_data\n",
    "\n",
    "plt.rcParams['figure.figsize']=(8.0,6.0)    #(6.0,4.0)\n",
    "figure(num=None, figsize=(12, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.rcParams['font.size']=12                #10 \n",
    "plt.rcParams['savefig.dpi']=100             #72 \n",
    "plt.rcParams['figure.subplot.bottom']=.1 \n",
    "\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        max_words=200,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(data))\n",
    "    \n",
    "    fig = plt.figure(1, figsize=(8, 8))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "show_wordcloud(df2['CleanedText'])\n",
    "df2.loc[df2['SentimentPolarity'] == 'Positive']['CleanedText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading/Selecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the SQLite Table to read data.\n",
    "con = sqlite3.connect('database.sqlite') \n",
    "\n",
    "#filtering only positive and negative reviews i.e. \n",
    "# not taking into consideration those reviews with Score=3 or regarding them as neutral\n",
    "# SELECT * FROM Reviews WHERE Score != 3 LIMIT 500000, will give top 500000 data points\n",
    "# you can change the number to any other number based on your computing power\n",
    "\n",
    "filtered_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 LIMIT 5000\"\"\", con) \n",
    "\n",
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "#changing reviews with score less than 3 to be positive and vice-versa\n",
    "actualScore = filtered_data['Score']\n",
    "positiveNegative = actualScore.map(partition) \n",
    "filtered_data['Score'] = positiveNegative\n",
    "print(\"Number of data points in our data\", filtered_data.shape)\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning: Deleting duplicate values\n",
    "It is observed that the reviews data have many duplicate entries. Hence it was necessary to remove duplicates in order to get unbiased results for the analysis of the data.\n",
    "\n",
    "Helpfulness Numerator: Number of Peoples who found the review helpful to them.\n",
    "\n",
    "Helpfulness Denominator: Number of Peoples indicated whether they found the review helpful or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE Score != 3 AND UserId=\"AR5J8UI46CURR\"\n",
    "ORDER BY ProductID\n",
    "\"\"\", con)\n",
    "evaluate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://miro.medium.com/max/756/1*EyU641UXrBiTrdf10OlORg.png\", width=600, height=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://miro.medium.com/max/756/1*qO48JZ3MamjxmusG3Qot9w.png\", width=600, height=600)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in these two images the brand name brand is same for the products, only flavor is different.\n",
    "\n",
    "so we have to make a code where the method ensures that there is only one representative for each product and deduplication without sorting would lead to possibility of different representatives still existing for the same product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can observe above that the same user has multiple reviews of the product with the same values for HelpfulnessDenominator,HelpfulnessNumerator, Score, Time, Text and Summary and on doing analysis it was found that\n",
    "\n",
    "ProductId(B000HDOPZG),(B000PAQ75C) was Loacker Quadratini Vanilla Wafer Cookies(Pack of 8) and so on\n",
    "\n",
    " In order to reduce redundancy and make it les complex it was decided to eliminate the rows having same parameters(except ProductId)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting data according to ProductId in ascending order\n",
    "sorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deduplication of entries\n",
    "final_result=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n",
    "final_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how much % of data still remains\n",
    "(final_result['Id'].size*1.0)/(filtered_data['Id'].size*1.0)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = final_result[final_result.HelpfulnessNumerator<=final_result.HelpfulnessDenominator]  ## the value of HelpfulnessNumerator is greater than HelpfulnessDenominator which is not possible hence these rows are removed from calcualtions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the number of entries left\n",
    "print(final_result.shape)\n",
    "\n",
    "#How many positive and negative reviews are present \n",
    "final_result['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=set(stopwords.words('english'))#set of stop words\n",
    "sno=nltk.stem.SnowballStemmer('english') #set of snow ball stemmers in english\n",
    "\n",
    "def cleanhtml(sentence): #function to clean html tags in a sentence\n",
    "    cleannr=re.compile('<.*?>')\n",
    "    comptext=re.sub(cleannr,'',sentence)\n",
    "    return comptext\n",
    "\n",
    "def cleanpunc(sentence) : #function to clean punctuation in the sentence\n",
    "    cleaned=re.sub(r'[? | ! | \\' |\" | #]',r'',sentence)\n",
    "    cleaned=re.sub(r'[. | , | ) | ( | \\ | / ]' ,r' ',sentence)\n",
    "    return cleaned\n",
    "\n",
    "print(stop)\n",
    "print('**' * 50)\n",
    "print(sno.stem('taste'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will check the distribution of stemmed word lengths across the whole review dataset to understand what is the length of the maximum number of words we will consider for the word to be relevant.\n",
    "#In other words we will keep only those words which has a length less than that of a speicific length (we will obtain this specific length from the histogram).\n",
    "total_words = []\n",
    "\n",
    "for review in tqdm(final_data['Text'].values):\n",
    "    filtered_sentence=[]\n",
    "    review = decontracted(review)\n",
    "    review = removeNumbers(review)\n",
    "    review = removeHtml(review)\n",
    "    review = removeURL(review)\n",
    "    review = removePunctuations(review)\n",
    "    review = removePatterns(review)\n",
    "    \n",
    "    for cleaned_words in review.split():   \n",
    "        if((cleaned_words not in custom_stopwords)):          \n",
    "            stemed_word=(sno.stem(cleaned_words.lower()))\n",
    "            total_words.append(stemed_word)\n",
    "\n",
    "total_words = list(set(total_words)) #Get list of unique words.\n",
    "\n",
    "#A list to hold the length of each words used in all the reviews used across the whole dataset.\n",
    "dist = []\n",
    "for i in tqdm(total_words):\n",
    "    length = len(i)\n",
    "    dist.append(length)\n",
    "\n",
    "# matplotlib histogram to see the distribution of the length of words\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.hist(dist, color = 'red', edgecolor = 'blue', bins =90)\n",
    "plt.title('Distribution of the length of Words across all reviews.')\n",
    "plt.xlabel('Word Lengths')\n",
    "plt.ylabel('Number of Words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove urls from text python: https://stackoverflow.com/a/40823105/4084039\n",
    "# printing some random reviews\n",
    "sent_0 = final_result['Text'].values[0]\n",
    "print(sent_0)\n",
    "print(\"=\"*50)\n",
    "\n",
    "sent_1000 = final_result['Text'].values[1000]\n",
    "print(sent_1000)\n",
    "print(\"=\"*50)\n",
    "\n",
    "sent_1400 = final_result['Text'].values[1400]\n",
    "print(sent_1400)\n",
    "print(\"=\"*50)\n",
    "\n",
    "sent_4900 = final_result['Text'].values[4900]\n",
    "print(sent_4900)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "\n",
    "def deactivate(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/16206380/python-beautifulsoup-how-to-remove-all-tags-from-an-element\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(sent_0, 'lxml')\n",
    "text = soup.get_text()\n",
    "print(text)\n",
    "print(\"=\"*50)\n",
    "\n",
    "soup = BeautifulSoup(sent_1000, 'lxml')\n",
    "text = soup.get_text()\n",
    "print(text)\n",
    "print(\"=\"*50)\n",
    "\n",
    "soup = BeautifulSoup(sent_1400, 'lxml')\n",
    "text = soup.get_text()\n",
    "print(text)\n",
    "print(\"=\"*50)\n",
    "\n",
    "soup = BeautifulSoup(sent_4900, 'lxml')\n",
    "text = soup.get_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1400 = deactivate(sent_1400)\n",
    "print(sent_1400)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#remove words with numbers python: https://stackoverflow.com/a/18082370/4084039\n",
    "sent_1400 = re.sub(\"\\S*\\d\\S*\", \"\", sent_1400).strip()\n",
    "print(sent_1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove spacial character: https://stackoverflow.com/a/5843547/4084039\n",
    "sent_1400 = re.sub('[^A-Za-z0-9]+', ' ', sent_1400)\n",
    "print(sent_1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/sebleier/554280\n",
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "# <br /><br /> ==> after the above steps, we are getting \"br br\"\n",
    "# we are including them into stop words list\n",
    "# instead of <br /> if we have <br/> these tags would have revmoved in the 1st step\n",
    "\n",
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for step by step implementing text preprocess\n",
    "#this code is takes time because it needs to run on 500 k sentences.\n",
    "## Combining all the above stundents \n",
    "from tqdm import tqdm # tqdm is for printing the status bar\n",
    "i=0\n",
    "str1=' '\n",
    "preprocessed_reviews=[]\n",
    "all_positive_words=[] #store words from +ve reviews here\n",
    "all_negative_words=[] # store words from -ve reviews here\n",
    "s=''\n",
    "\n",
    "for sentance in tqdm(final_result['Text'].values):\n",
    "    filtered_sentence=[]\n",
    "    #print(sentance)\n",
    "    sentance=cleanhtml(sentance)#removing the html tags\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = deactivate(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    # sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    for w in sentance.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words) > 2)):\n",
    "                if (cleaned_words.lower() not in stopwords):\n",
    "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if (final_result['Score'].values[i]) == 1 :\n",
    "                      all_positive_words.append(s) #list all the positive words\n",
    "                    if (final_result['Score'].values[i]) == 0 :\n",
    "                       all_negative_words.append(s) #list all the negative words\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "    #print(filtered_sentence)\n",
    "    str1=b\" \".join(filtered_sentence) #final string of the filtered sentence\n",
    "    \n",
    "    preprocessed_reviews.append(str1.strip())\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result['preprocessed_reviews']=preprocessed_reviews #adding cleaned text to the\n",
    "final_result['preprocessed_reviews']=final_result['preprocessed_reviews'].str.decode(\"utf-8\")\n",
    "final_result['preprocessed_reviews'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.to_csv('preprocessed_reviews',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conn=sqlite3.connect('final.sqlite')\n",
    "#c=conn.cursor()\n",
    "#conn.text_factory=str\n",
    "#final.to_sql('pre_text',conn,schema=None,if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for step by step implementing text preprocess\n",
    "#this code is takes time because it needs to run on 50000 sentences.\n",
    "from tqdm import tqdm # tqdm is for printing the status bar\n",
    "i=0\n",
    "str1=' '\n",
    "preprocessed_summary=[]\n",
    "all_positive_summary=[] #store words from +ve reviews here\n",
    "all_negative_summary=[] # store words from -ve reviews here\n",
    "s=''\n",
    "\n",
    "for sentance in tqdm(final_result['Summary'].values):\n",
    "    filtered_summary=[]\n",
    "    #print(sentance)\n",
    "    sentance=cleanhtml(sentance)#removing the html tags\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = deactivate(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    # sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    for w in sentance.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words) > 2)):\n",
    "                if (cleaned_words.lower() not in stopwords):\n",
    "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_summary.append(s)\n",
    "                    if (final_result['Score'].values[i]) == 1 :\n",
    "                      all_positive_summary.append(s) #list all the positive words\n",
    "                    if (final_result['Score'].values[i]) == 0 :\n",
    "                      all_negative_summary.append(s) #list all the negative words\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "    #print(filtered_sentence)\n",
    "    str1=b\" \".join(filtered_summary) #final string of the filtered sentence\n",
    "    \n",
    "    preprocessed_summary.append(str1.strip())\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result['preprocessed_summary']=preprocessed_summary #adding cleaned text to the\n",
    "final_result['preprocessed_summary']=final_result['preprocessed_summary'].str.decode(\"utf-8\")\n",
    "final_result['preprocessed_summary'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.to_csv('preprocessed_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store all in to database for future\n",
    "#conn=sqlite3.connect('preprocessed.sqlite')\n",
    "#c=conn.cursor()\n",
    "#conn.text_factory=str\n",
    "#final.to_sql('preprocessed_data',conn,schema=None,if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_summary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_negative_words[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_negative_summary[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Score.value_counts().sort_index().plot.bar(alpha=0.7, grid=True, color = 'orange', width = 0.9)\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Number Of Reviews')\n",
    "plt.title('Distribution of reviews over each score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the plot that more than 80000 reviews have given 4 score and around 30000 have given 2 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many empty length texts are present in the dataset()\n",
    "df[df['Text']==0].Text.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " this means there are no empty rows of text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower() #converting text to lowercase\n",
    "    text = ' '.join([i for i in nltk.word_tokenize(text) if i not in stopwords_en and i not in string.punctuation]) #stopword and punct removal\n",
    "    text = re.sub('[^a-z]+', ' ', text) #removal of anything other than English letters\n",
    "    text = ' '.join([ps.stem(i) for i in nltk.word_tokenize(text)]) #stemming\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will apply all the basic steps that I have discussed above and finally we collect the words used to describe positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing some random reviews\n",
    "sent_0 = final_result['Text'].values[0]\n",
    "print(sent_0)\n",
    "print(\"=\"*50)\n",
    "\n",
    "sent_1001 = final_result['Text'].values[1000]\n",
    "print(sent_1001)\n",
    "print(\"=\"*50)\n",
    "\n",
    "sent_1400 = final_result['Text'].values[1500]\n",
    "print(sent_1400)\n",
    "print(\"=\"*50)\n",
    "\n",
    "sent_4560 = final_result['Text'].values[4900]\n",
    "print(sent_4560)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def deactivate(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "# <br /><br /> ==> after the above steps, we are getting \"br br\"\n",
    "# we are including them into stop words list\n",
    "# instead of <br /> if we have <br/> these tags would have removed in the 1st step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning for all the reviews \n",
    "from bs4 import BeautifulSoup   #Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree.\n",
    "                                #It commonly saves programmers hours or days of work.\n",
    "\n",
    "from tqdm import tqdm\n",
    "pp_reviews = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentence in tqdm(final_result['Text'].values):\n",
    "    sentence = re.sub(r\"http\\S+\", \"\", sentence)           #remove urls\n",
    "    sentence = BeautifulSoup(sentence, 'lxml').get_text() #remove xml tags\n",
    "    sentence = deactivate(sentence)                     \n",
    "    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()   #remove words with numbers\n",
    "    sentence = re.sub('[^A-Za-z]+', ' ', sentence)        #remove special characters\n",
    "    sentence = ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopwords)\n",
    "    pp_reviews.append(sentence.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_reviews[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pp_reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import SpatialDropout1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(pp_reviews)\n",
    "vocab_size = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "code_docs = t.texts_to_sequences(pp_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad documents to a max length of max words\n",
    "max_length = 150\n",
    "pad_docs = pad_sequences(code_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = final_result['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_docs[1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower() #converting text to lowercase\n",
    "    text = ' '.join([i for i in nltk.word_tokenize(text) if i not in stopwords_en and i not in string.punctuation]) #stopword and punct removal\n",
    "    text = re.sub('[^a-z]+', ' ', text) #removal of anything other than English letters\n",
    "    text = ' '.join([ps.stem(i) for i in nltk.word_tokenize(text)]) #stemming\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleanup and create a new column\n",
    "df['FilterText'] = df['Text'].apply(lambda x: clean_text(x))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(val):\n",
    "    if(val>2):\n",
    "        return 1\n",
    "    return 0\n",
    "df['Positivity']=df['Score'].apply(lambda x: partition(x))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_columns = ['FilterText', 'Positivity']\n",
    "df = df[required_columns]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Positivity.value_counts().plot.bar(alpha=0.5, grid=True)\n",
    "plt.title('Distribution Of Positive & Negative Feedback')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 120000 people have given positive reviews(1) and around 80000 people have given negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train/Test data split model\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['FilterText'], df['Positivity'], test_size=0.25, random_state=42, shuffle=True, stratify=df['Positivity'])\n",
    "print(\"Train Set Size = {}\\nTest Set Size  = {}\".format(X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW\n",
    "count_vect = CountVectorizer() #in scikit-learn\n",
    "count_vect.fit(preprocessed_reviews)\n",
    "print(\"some feature names \", count_vect.get_feature_names()[500:510])\n",
    "print('='*50)\n",
    "\n",
    "final_counts = count_vect.transform(pp_reviews)\n",
    "print(\"the type of count vectorizer \",type(final_counts))\n",
    "print(\"the shape of out text BOW vectorizer \",final_counts.get_shape())\n",
    "print(\"the number of unique words \", final_counts.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('preprocessed_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['preprocessed_reviews'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bi-gram, tri-gram and n-gram\n",
    "#removing stop words like \"not\" should be avoided before building n-grams\n",
    "# count_vect = CountVectorizer(ngram_range=(1,2))\n",
    "# you can choose these numbers min_df=10, max_features=5000\n",
    "count_vect = CountVectorizer(ngram_range=(1,2), min_df=10, max_features=5000)\n",
    "final_bigram_counts = count_vect.fit_transform(data['preprocessed_reviews'])[:5000]\n",
    "print(\"the type of count vectorizer \",type(final_bigram_counts))\n",
    "print(\"the shape of out text BOW vectorizer \",final_bigram_counts.get_shape())\n",
    "print(\"the number of unique words including both unigrams and bigrams \", final_bigram_counts.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF(Term frequency -inverse document frequency):\n",
    "    \n",
    "TF-IDF stands for “Term Frequency — Inverse Document Frequency”. This is a technique to quantify a word in documents, we generally compute a weight to each word which signifies the importance of the word in the document and corpus. This method is a widely used technique in Information Retrieval and Text Mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=10)\n",
    "tf_idf_vect.fit(data['preprocessed_reviews'])\n",
    "print(\"some sample features(unique words in the corpus)\",tf_idf_vect.get_feature_names()[0:10])\n",
    "print('='*50)\n",
    "\n",
    "final_tf_idf = tf_idf_vect.transform(data['preprocessed_reviews'])\n",
    "print(\"the type of count vectorizer \",type(final_tf_idf))\n",
    "print(\"the shape of out text TFIDF vectorizer \",final_tf_idf.get_shape())\n",
    "print(\"the number of unique words including both unigrams and bigrams \", final_tf_idf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec:\n",
    "This technique is the state of the art algorithm, it consider the semantic meaning of the word.\n",
    "If we give the word it converts in to vectors. It also learns relationship automatically from the text.\n",
    "The output of the word2vec model is Dense vectors.Word2vec model requires large text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('preprocessed_data.csv')\n",
    "data['preprocessed_reviews']=data['preprocessed_reviews'].fillna(method='bfill')\n",
    "data['preprocessed_reviews'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your own Word2Vec model using your own text corpus\n",
    "i=0\n",
    "list_of_sentance=[]\n",
    "for sentance in data['preprocessed_reviews']:\n",
    "    list_of_sentance.append(sentance.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentance[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Google News Word2Vectors\n",
    "# in this project we are using a pretrained model by google \n",
    "#  a pickle file wich contains a dict , \n",
    "# and it contains all our courpus words as keys and  model[word] as values\n",
    "# To use this code-snippet, download \"GoogleNews-vectors-negative300.bin\" \n",
    "# from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "is_your_ram_gt_16g=False\n",
    "want_to_use_google_w2v = False\n",
    "want_to_train_w2v = True\n",
    "\n",
    "if want_to_train_w2v:\n",
    "    # min_count = 5 considers only words that occured atleast 5 times\n",
    "    w2v_model=Word2Vec(list_of_sentance,min_count=5, workers=4)\n",
    "    print(w2v_model.wv.most_similar('great'))\n",
    "    print('='*50)\n",
    "    print(w2v_model.wv.most_similar('worst'))\n",
    "    \n",
    "elif want_to_use_google_w2v and is_your_ram_gt_16g:\n",
    "    if os.path.isfile('GoogleNews-vectors-negative300.bin'):\n",
    "        w2v_model=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "        print(w2v_model.wv.most_similar('great'))\n",
    "        print(w2v_model.wv.most_similar('worst'))\n",
    "    else:\n",
    "        print(\"you don't have gogole's word2vec file, keep want_to_train_w2v = True, to train your own w2v \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectoriser = TfidfVectorizer()\n",
    "\n",
    "# Training % Feature Extraction On Entire Dataset, Used For Cross Validation & Model Comparison\n",
    "features = tfidf_vectoriser.fit_transform(df['FilterText'])\n",
    "labels = df['Positivity'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training On Only Train Set Now\n",
    "tfidf_vectoriser.fit(X_train)\n",
    "X_train_tf = tfidf_vectoriser.transform(X_train)\n",
    "X_test_tf = tfidf_vectoriser.transform(X_test)\n",
    "X_train_tf.shape, X_test_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "print(\"Twenty Random Words from Training Set ...\\n\",*random.sample(tfidf_vectoriser.get_feature_names(),20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work. Parameters alphafloat, default=1.0.\n",
    "from sklearn.svm import LinearSVC #The objective of a Linear SVC (Support Vector Classifier) is to fit to the data you provide, returning a \"best fit\" hyperplane that divides, or categorizes, your data\n",
    "from sklearn.model_selection import cross_val_score #Cross-validation is a statistical method used to estimate the skill of machine learning models.That k-fold cross validation is a procedure used to estimate the skill of the model on new data. There are common tactics that you can use to select the value of k for your dataset.\n",
    "from sklearn.linear_model import LogisticRegression #Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable.\n",
    "from sklearn.ensemble import RandomForestClassifier #A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "    RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=0)\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(parameters)))\n",
    "entries = []\n",
    "for model in parameters:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, features ,labels, scoring='accuracy', cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'C':[0.5,0.8,1.0,1.5]\n",
    "}\n",
    "svm = LinearSVC(max_iter=1500)\n",
    "svm_cv = GridSearchCV(svm, param_grid, cv=5)\n",
    "svm_cv.fit(features, labels)\n",
    "print(\"Best Parameters :\", svm_cv.best_params_)\n",
    "print(\"Amazing Score :\",svm_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(C=0.5, max_iter=2000)\n",
    "svm.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "y_pred = svm.predict(X_test_tf)\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative','Positive']))\n",
    "print(\"Accuracy :\",accuracy_score(y_test, y_pred), end='\\n\\n')\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data using Spacy and Machine learning model training using sklearn\n",
    "\n",
    "In this stage, Spacy package of python is used to lemmatize and remove stop words from the obtained dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  spacy.lang.en.stop_words import STOP_WORDS\n",
    "# To build a list of stop words for filtering\n",
    "stopwords = list(STOP_WORDS)\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the stop words have been enlisted\n",
    "\n",
    "The data is initially split into test and training datasets prior to feeding into the machine learning pipeline. Then, a class object was defined as 'sent_predict' is used as the first step of the pipeline which would inherit from the TransformerMixin package and perform the cleaning of data. The second method of the pipeline is to vectorize the cleaned data.\n",
    "\n",
    "Tokenized words needs to be lemmatized and filtered for pronouns, stopwords and punctuations using the defined method 'my_tokenizer' For that purpose count vectorizeor and tfidfVectorizer both have been tried subsequently to decide which is better.\n",
    "\n",
    "Then the third step of the pipeline is the defining of the classifier. In this case, Linear Support Vector Machine classifier was chosen. Other methods could be explored in the furture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = string.punctuation\n",
    "# Creating a Spacy Parser\n",
    "from spacy.lang.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.base import TransformerMixin \n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the  pipeline to clean, tokenize, vectorize, and classify using\"Count Vectorizor\"\n",
    "pipe_countvect = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', vectorizer),\n",
    "                 ('classifier', classifier)])\n",
    "# Fit our data\n",
    "pipe_countvect.fit(X_train,y_train)\n",
    "# Predicting with a test dataset\n",
    "sample_prediction = pipe_countvect.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom transformer using spaCy \n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [clean_text(text) for text in X]\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text \n",
    "def clean_text(text):     \n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.base import TransformerMixin \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "vectorizer = CountVectorizer(tokenizer = my_tokenizer, ngram_range=(1,1)) \n",
    "classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Tfidf\n",
    "tfvectorizer = TfidfVectorizer(tokenizer = my_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('pp_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and Labels\n",
    "X = df1['Text']\n",
    "ylabels = df1['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', vectorizer),\n",
    "                 ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prediction = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHTMLTags(review):\n",
    "    soup = BeautifulSoup(review, 'lxml')\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeApostrophe(review):\n",
    "    phrase = re.sub(r\"won't\", \"will not\", review)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", review)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", review)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", review)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", review)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", review)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", review)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", review)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", review)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", review)\n",
    "    return phrase\n",
    "\n",
    "def removeAlphaNumericWords(review):\n",
    "     return re.sub(\"\\S*\\d\\S*\", \"\", review).strip()\n",
    " \n",
    "def removeSpecialChars(review):\n",
    "     return re.sub('[^a-zA-Z]', ' ', review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doTextCleaning(review):\n",
    "    review = removeHTMLTags(review)\n",
    "    review = removeApostrophe(review)\n",
    "    review = removeAlphaNumericWords(review)\n",
    "    review = removeSpecialChars(review) \n",
    "    # Lower casing\n",
    "    review = review.lower()  \n",
    "    #Tokenization\n",
    "    review = review.split()\n",
    "    #Removing Stopwords and Lemmatization\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    review = [lmtzr.lemmatize(word, 'v') for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = \" \".join(review)    \n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorePartition(x):\n",
    "    if x < 3:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=final_result.sort_values(by=['Time'],ascending=False)\n",
    "finalDataPoints=final_result.head(100000)\n",
    "x=finalDataPoints[\"Text\"]\n",
    "y=finalDataPoints[\"Score\"]\n",
    "\n",
    "x_tr,x_test,y_tr,y_test=train_test_split(x, y, test_size=0.2,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOW for unigram\n",
    "bow = CountVectorizer()\n",
    "x_tr_uni = bow.fit_transform(x_tr)\n",
    "x_test_uni= bow.transform(x_test)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x_tr_uni = StandardScaler(with_mean = False).fit_transform(x_tr_uni)\n",
    "x_test_uni = StandardScaler(with_mean = False).fit_transform(x_test_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn 0.21.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(penalty='l2')\n",
    "C_value = [{'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}]\n",
    "gsv = GridSearchCV(LR,C_value,cv=5,verbose=1,scoring='f1_weighted')\n",
    "gsv.fit(x_tr_uni,y_tr)\n",
    "print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "print(gsv.best_score)\n",
    "optimal_C=gsv.best_score\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "plt.figure(figsize=(8,8))\n",
    "for a in gsv.cv_scores_:\n",
    "    x.append(a[0]['C']) \n",
    "    y.append(a[1])\n",
    "plt.xlabel(\"C\",fontsize=15)\n",
    "plt.ylabel(\"f1_weighted\")\n",
    "plt.title('f1_weighted v/s C')\n",
    "plt.plot(x,y, marker='o', markerfacecolor='red', markersize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the performance of model both on train data and cross validation data for each hyper parameter. Display the confusion matrix as well as ROC Curve\n",
    "#From the heatmaps above, we will select the optimal value of max_depth to be 25 and min_samples_split to be 500.\n",
    "max_depth = 25\n",
    "min_samples_split=500,\n",
    "\n",
    "best_estimator = DecisionTreeClassifier(criterion='gini', max_depth=25, min_samples_split=500, random_state=0, splitter='best')\n",
    "vectorizationType=\"TFIDF\" \n",
    "trained_clf_TFIDF = performance(best_estimator, vectorizationType, X_train, y_train, X_test, y_test, X_calib, y_calib, max_depth, min_samples_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Call the function above and pass a filename onto it.\n",
    "f_names=tf_idf_obj.get_feature_names()\n",
    "\n",
    "graph=visualize_tree(trained_clf_TFIDF, f_names, 'TFIDF_DT.png')\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the W2V Vectors we had created earlier and standardize them. We will standardize the traina and test data seperately in order to prevent data leakage.\n",
    "import pickle\n",
    "\n",
    "with open('X_train_W2V.pkl', 'rb') as file:\n",
    "    X_train = pickle.load(file)\n",
    "    \n",
    "with open('X_test_W2V.pkl', 'rb') as file:\n",
    "    X_test = pickle.load(file)\n",
    "\n",
    "with open('y_train_W2V.pkl', 'rb') as file:\n",
    "    y_train = pickle.load(file)\n",
    "    \n",
    "with open('y_test_W2V.pkl', 'rb') as file:\n",
    "    y_test = pickle.load(file)\n",
    "    \n",
    "with open('X_calib_W2V.pkl', 'rb') as file:\n",
    "    X_calib = pickle.load(file)\n",
    "\n",
    "with open('y_calib_W2V.pkl', 'rb') as file:\n",
    "    y_calib = pickle.load(file)    \n",
    "\n",
    "\n",
    "print(\"Shape of the train data matrix: \",X_train.shape)\n",
    "print(\"Shape of the test data matrix: \",X_test.shape)\n",
    "print(\"Shape of the calibration data matrix: \",X_calib.shape)\n",
    "\n",
    "#Perform Grid Search cross validation to obtain the best value of the hyperparameter.\n",
    "vectorizationType = \"AVG-W2V\"\n",
    "st=datetime.now()\n",
    "gsearch_cv = get_GridSearchCV(vectorizationType, X_train, y_train, X_test, y_test)\n",
    "print(\"\\nTime taken to complete Hyperparameter Search: \",datetime.now()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the performance of model both on train data and cross validation data for each hyper parameter. Display the confusion matrix as well as ROC Curve\n",
    "#From the heatmaps above, we will select the optimal value of max_depth to be 10 and min_samples_split to be 250.\n",
    "max_depth = 10\n",
    "min_samples_split=250,\n",
    "\n",
    "best_estimator = DecisionTreeClassifier(criterion='gini', max_depth=10, min_samples_split=250, random_state=0, splitter='best')\n",
    "vectorizationType=\"AVG-W2V\" \n",
    "trained_clf_avg_w2v = performance(best_estimator, vectorizationType, X_train, y_train, X_test, y_test, X_calib, y_calib, max_depth, min_samples_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the TF-IDF W2V Vectors we had created earlier and standardize them. We will standardize the traina and test data seperately in order to prevent data leakage.\n",
    "import pickle\n",
    "    \n",
    "with open('X_train_TFIDF-W2V.pkl', 'rb') as file:\n",
    "    X_train = pickle.load(file)\n",
    "    \n",
    "with open('X_test_TFIDF-W2V.pkl', 'rb') as file:\n",
    "    X_test = pickle.load(file)\n",
    "\n",
    "with open('y_train_TFIDF-W2V.pkl', 'rb') as file:\n",
    "    y_train = pickle.load(file)\n",
    "    \n",
    "with open('y_test_TFIDF-W2V.pkl', 'rb') as file:\n",
    "    y_test = pickle.load(file)\n",
    "    \n",
    "with open('X_calib_TFIDF-W2V.pkl', 'rb') as file:\n",
    "    X_calib = pickle.load(file)\n",
    "\n",
    "with open('y_calib_TFIDF-W2V.pkl', 'rb') as file:\n",
    "    y_calib = pickle.load(file)    \n",
    "\n",
    "print(\"Shape of the train data matrix: \",X_train.shape)\n",
    "print(\"Shape of the test data matrix: \",X_test.shape)\n",
    "print(\"Shape of the calibration data matrix: \",X_calib.shape)\n",
    "\n",
    "#Perform Grid Search cross validation to obtain the best value of the hyperparameter.\n",
    "vectorizationType = \"TFIDF-W2V\"\n",
    "st=datetime.now()\n",
    "gsearch_cv = get_GridSearchCV(vectorizationType, X_train, y_train, X_test, y_test)\n",
    "print(\"\\nTime taken to complete Hyperparameter Search: \",datetime.now()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the performance of model both on train data and cross validation data for each hyper parameter. Display the confusion matrix as well as ROC Curve\n",
    "#From the heatmaps above, we will select the optimal value of max_depth to be 10 and min_samples_split to be 500.\n",
    "max_depth = 10\n",
    "min_samples_split=500,\n",
    "\n",
    "best_estimator = DecisionTreeClassifier(criterion='gini', max_depth=10, min_samples_split=500, random_state=0, splitter='best')\n",
    "vectorizationType=\"TFIDF-W2V\" \n",
    "trained_clf_avg_w2v = performance(best_estimator, vectorizationType, X_train, y_train, X_test, y_test, X_calib, y_calib, max_depth, min_samples_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Compare performance and display it on a pretty table.\n",
    "from prettytable import PrettyTable\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Model\", \"Max Depth\", \"Min Samples Split\", \"Accuracy on Test data\", \"AUC Score Test Data\", \"No. Of accurate predictions\"]\n",
    "\n",
    "print(\"Please find below the important metrics for all the models below.\\n\")\n",
    "file = open('info_model_DT.txt', 'r')\n",
    "file.seek(0)\n",
    "for line in file:\n",
    "    table.add_row(line.split())\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
